# ğŸ”— IntegraÃ§Ã£o com CatÃ¡logo Databricks - SCGAS

## ğŸ“‹ VisÃ£o Geral

Este documento descreve a implementaÃ§Ã£o da funcionalidade para processar dados JSON da API SCGAS e salvÃ¡-los no catÃ¡logo `shd_qas_internal_datalake.scgas_raw` do Databricks.

## ğŸ¯ Objetivos

- **Processar dados JSON** coletados da API SCGAS
- **Criar tabela estruturada** no catÃ¡logo Databricks
- **Implementar schema flexÃ­vel** baseado em configuraÃ§Ã£o
- **Garantir integridade** dos dados salvos
- **Fornecer monitoramento** e logs detalhados

## ğŸ—ï¸ Arquitetura

### Pipeline Atualizado

```
API SCGAS â†’ AutenticaÃ§Ã£o â†’ Coleta â†’ Pandas â†’ Spark â†’ CatÃ¡logo Databricks
                                    â†“
                              save_to_databricks_catalog_node (NOVO!)
```

### Novos Componentes

1. **`save_to_databricks_catalog`** - FunÃ§Ã£o principal de salvamento
2. **`databricks_catalog.yml`** - ConfiguraÃ§Ã£o do catÃ¡logo
3. **`catalog_save_result`** - Dataset de saÃ­da com resultados
4. **Testes automatizados** - ValidaÃ§Ã£o da funcionalidade

## ğŸ“ Estrutura de Arquivos

```
scgas/
â”œâ”€â”€ src/scgas/pipelines/data_engineering/
â”‚   â”œâ”€â”€ nodes.py                    # âœ… Atualizado com nova funÃ§Ã£o
â”‚   â””â”€â”€ pipeline.py                 # âœ… Atualizado com novo node
â”œâ”€â”€ conf/base/
â”‚   â”œâ”€â”€ catalog.yml                 # âœ… Atualizado com novo dataset
â”‚   â””â”€â”€ databricks_catalog.yml      # ğŸ†• Nova configuraÃ§Ã£o
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_catalog_integration.py # ğŸ†• Novos testes
â””â”€â”€ notebooks/
    â””â”€â”€ 03_catalog_integration.py   # ğŸ†• Exemplo de uso
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo `databricks_catalog.yml`

```yaml
databricks_catalog:
  catalog_name: "shd_qas_internal_datalake"
  table_name: "scgas_raw"
  
  schema:
    - name: "codVar"
      type: "string"
      nullable: true
      description: "CÃ³digo da variÃ¡vel"
    # ... outros campos
    
  write_options:
    mode: "append"
    merge_schema: true
    partition_by: []
  
  performance:
    coalesce_partitions: true
    adaptive_query_execution: true
```

### Schema da Tabela

| Campo | Tipo | Nullable | DescriÃ§Ã£o |
|-------|------|----------|-----------|
| `codVar` | string | true | CÃ³digo da variÃ¡vel |
| `tag` | string | true | Tag de identificaÃ§Ã£o |
| `idIntegracao` | string | true | ID de integraÃ§Ã£o |
| `unidade` | string | true | Unidade de medida |
| `descricao` | string | true | DescriÃ§Ã£o da mediÃ§Ã£o |
| `data` | timestamp | true | Data/hora da mediÃ§Ã£o |
| `valorConv` | double | true | Valor convertido |
| `valorConvFormat` | double | true | Valor formatado |
| `estacao` | string | true | EstaÃ§Ã£o de mediÃ§Ã£o |
| `codEst` | string | true | CÃ³digo da estaÃ§Ã£o |
| `codMed` | string | true | CÃ³digo da mediÃ§Ã£o |
| `intervaloLeituraMin` | integer | true | Intervalo de leitura |
| `data_coleta` | timestamp | true | Data/hora da coleta |
| `processamento_timestamp` | timestamp | true | Data/hora do processamento |

## ğŸš€ Como Usar

### 1. ExecuÃ§Ã£o do Pipeline Completo

```bash
# Executa todo o pipeline incluindo salvamento no catÃ¡logo
kedro run --pipeline=data_engineering
```

### 2. ExecuÃ§Ã£o Apenas do Salvamento

```bash
# Executa apenas o node de salvamento no catÃ¡logo
kedro run --nodes=save_to_databricks_catalog_node
```

### 3. ExecuÃ§Ã£o via Python

```python
from scgas.pipelines.data_engineering.nodes import save_to_databricks_catalog

# Salva dados no catÃ¡logo
result = save_to_databricks_catalog(
    measurements_data, 
    api_config, 
    databricks_catalog_config
)

print(f"Status: {result['status']}")
print(f"Registros processados: {result['records_processed']}")
```

## ğŸ“Š Monitoramento e Logs

### Logs de ExecuÃ§Ã£o

```
ğŸ”„ Iniciando processamento para catÃ¡logo Databricks...
âœ… Conectado ao Spark: 3.4.0
   AplicaÃ§Ã£o: scgas-project
ğŸ¯ ConfiguraÃ§Ã£o do catÃ¡logo: shd_qas_internal_datalake.scgas_raw
ğŸ“Š Dados processados: 150 registros
âœ… DataFrame Spark criado: 150 linhas, 14 colunas
âœ… CatÃ¡logo shd_qas_internal_datalake verificado/criado
âœ… Schema default verificado/criado
âœ… Dados salvos com sucesso na tabela shd_qas_internal_datalake.scgas_raw
ğŸ“Š Total de registros na tabela: 150
```

### MÃ©tricas de SaÃ­da

```json
{
  "status": "success",
  "catalog": "shd_qas_internal_datalake",
  "table": "scgas_raw",
  "full_table_name": "shd_qas_internal_datalake.scgas_raw",
  "records_processed": 150,
  "total_records_in_table": 150,
  "schema": ["codVar", "tag", "valorConv", ...],
  "write_mode": "append",
  "merge_schema": true,
  "message": "Dados salvos com sucesso no catÃ¡logo shd_qas_internal_datalake.scgas_raw",
  "timestamp": "2025-01-01T10:00:00"
}
```

## ğŸ” VerificaÃ§Ã£o dos Dados

### Consulta SQL para Verificar

```sql
-- Conta total de registros
SELECT COUNT(*) as total_registros 
FROM shd_qas_internal_datalake.scgas_raw;

-- Mostra estrutura da tabela
DESCRIBE shd_qas_internal_datalake.scgas_raw;

-- Amostra dos dados
SELECT * FROM shd_qas_internal_datalake.scgas_raw 
LIMIT 5;

-- EstatÃ­sticas por estaÃ§Ã£o
SELECT 
    estacao,
    COUNT(*) as total_medicoes,
    AVG(valorConv) as valor_medio,
    MIN(valorConv) as valor_minimo,
    MAX(valorConv) as valor_maximo
FROM shd_qas_internal_datalake.scgas_raw
WHERE valorConv IS NOT NULL
GROUP BY estacao
ORDER BY total_medicoes DESC;
```

## ğŸ§ª Testes

### ExecuÃ§Ã£o dos Testes

```bash
# Executa todos os testes
pytest

# Executa apenas testes do catÃ¡logo
pytest tests/test_catalog_integration.py -v

# Executa com cobertura
pytest --cov=src/scgas tests/test_catalog_integration.py
```

### Cobertura de Testes

- âœ… **Sucesso no salvamento** - Valida salvamento correto
- âœ… **Schema padrÃ£o** - Testa fallback para schema padrÃ£o
- âœ… **Tratamento de erro** - Valida tratamento de falhas
- âœ… **ValidaÃ§Ã£o de configuraÃ§Ã£o** - Testa configuraÃ§Ãµes invÃ¡lidas

## âš ï¸ ConsideraÃ§Ãµes Importantes

### 1. **PermissÃµes do Databricks**
- UsuÃ¡rio deve ter permissÃ£o para criar catÃ¡logos e tabelas
- Cluster deve estar ativo e acessÃ­vel
- ConfiguraÃ§Ã£o de conectividade deve estar correta

### 2. **ConfiguraÃ§Ã£o de Credenciais**
- Credenciais da API SCGAS devem estar configuradas
- Token de acesso ao Databricks deve ser vÃ¡lido
- ConfiguraÃ§Ãµes locais devem estar em `conf/local/`

### 3. **Performance e Escalabilidade**
- Dados sÃ£o salvos em modo `append` por padrÃ£o
- Schema Ã© mesclado automaticamente (`mergeSchema: true`)
- Particionamento pode ser configurado se necessÃ¡rio

### 4. **Tratamento de Erros**
- Fallback para processamento pandas se Spark falhar
- Logs detalhados para debugging
- Retorno estruturado com status e mensagens

## ğŸ”„ PrÃ³ximos Passos

### Melhorias Planejadas

1. **Particionamento automÃ¡tico** por data
2. **CompressÃ£o otimizada** para diferentes tipos de dados
3. **ValidaÃ§Ã£o de dados** antes do salvamento
4. **MÃ©tricas de qualidade** dos dados salvos
5. **NotificaÃ§Ãµes** para falhas ou sucessos

### IntegraÃ§Ãµes Futuras

1. **Delta Lake** para versionamento de dados
2. **Unity Catalog** para governanÃ§a avanÃ§ada
3. **Streaming** para dados em tempo real
4. **MLflow** para experimentos de ML

## ğŸ“ Suporte

### Logs e Debugging

- Verifique logs do Kedro para detalhes de execuÃ§Ã£o
- Use `kedro viz` para visualizar o pipeline
- Consulte logs do cluster Databricks para erros de Spark

### Comandos Ãšteis

```bash
# Visualizar pipeline
kedro viz

# Ver configuraÃ§Ãµes
kedro info

# Executar com debug
kedro run --pipeline=data_engineering --verbose

# Limpar dados intermediÃ¡rios
kedro catalog clean
```

---

**VersÃ£o**: 1.0.0  
**Data**: Janeiro 2025  
**Autor**: Equipe de Engenharia de Dados  
**Status**: âœ… Implementado e Testado