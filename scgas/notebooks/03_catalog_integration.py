# ğŸ”— IntegraÃ§Ã£o com CatÃ¡logo Databricks - SCGAS
# 
# Este arquivo demonstra como os dados coletados da API SCGAS sÃ£o processados 
# e salvos no catÃ¡logo shd_qas_internal_datalake.scgas_raw.
# 
# Para usar como notebook, copie as cÃ©lulas para um notebook Jupyter.

# ============================================================================
# CÃ‰LULA 1: ImportaÃ§Ãµes e Setup
# ============================================================================

# Importa as funÃ§Ãµes necessÃ¡rias
from scgas.pipelines.data_engineering.nodes import (
    authenticate_scgas, 
    collect_measurements, 
    save_to_databricks_catalog
)
import json
from datetime import datetime

print("âœ… FunÃ§Ãµes importadas com sucesso")

# ============================================================================
# CÃ‰LULA 2: Carregamento de ConfiguraÃ§Ãµes
# ============================================================================

# Carrega configuraÃ§Ãµes
from kedro.framework.context import KedroContext
from kedro.framework.startup import bootstrap_project

# Inicializa o contexto do Kedro
bootstrap_project(".")
context = KedroContext(".")

# Carrega configuraÃ§Ãµes
api_config = context.config_loader["api_config"]
credentials = context.config_loader["credentials"]
databricks_catalog_config = context.config_loader["databricks_catalog"]

print("âœ… ConfiguraÃ§Ãµes carregadas")
print(f"API Base URL: {api_config['api_scgas']['scgas']['base_url']}")
print(f"CatÃ¡logo: {databricks_catalog_config['databricks_catalog']['catalog_name']}")
print(f"Tabela: {databricks_catalog_config['databricks_catalog']['table_name']}")

# ============================================================================
# CÃ‰LULA 3: AutenticaÃ§Ã£o na API SCGAS
# ============================================================================

# Autentica na API SCGAS
print("ğŸ” Autenticando na API SCGAS...")
auth_token = authenticate_scgas(api_config, credentials)
print(f"âœ… Token obtido: {auth_token[:20]}...")

# ============================================================================
# CÃ‰LULA 4: Coleta de Dados da API
# ============================================================================

# Coleta dados de mediÃ§Ã£o
print("ğŸ“¡ Coletando dados da API SCGAS...")
measurements_data = collect_measurements(auth_token, api_config)

print(f"âœ… Dados coletados: {len(measurements_data) if isinstance(measurements_data, list) else 'N/A'} registros")

# Mostra exemplo dos dados
if isinstance(measurements_data, list) and len(measurements_data) > 0:
    print("\nğŸ“Š Exemplo dos dados coletados:")
    print(json.dumps(measurements_data[0], indent=2, default=str))

# ============================================================================
# CÃ‰LULA 5: Salvamento no CatÃ¡logo Databricks
# ============================================================================

# Salva dados no catÃ¡logo Databricks
print("ğŸ’¾ Salvando dados no catÃ¡logo Databricks...")
catalog_result = save_to_databricks_catalog(
    measurements_data, 
    api_config, 
    databricks_catalog_config
)

print("\nğŸ“Š Resultado do salvamento:")
print(json.dumps(catalog_result, indent=2, default=str))

# ============================================================================
# CÃ‰LULA 6: VerificaÃ§Ã£o dos Dados Salvos
# ============================================================================

# Verifica os dados salvos usando Spark SQL
if catalog_result.get("status") == "success":
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder.getOrCreate()
    
    # Consulta os dados salvos
    full_table_name = f"{catalog_result['catalog']}.{catalog_result['table']}"
    
    print(f"ğŸ” Consultando dados da tabela: {full_table_name}")
    
    # Conta total de registros
    count_result = spark.sql(f"SELECT COUNT(*) as total FROM {full_table_name}")
    total_count = count_result.collect()[0]["total"]
    print(f"ğŸ“Š Total de registros na tabela: {total_count}")
    
    # Mostra estrutura da tabela
    print("\nğŸ—ï¸ Estrutura da tabela:")
    schema_result = spark.sql(f"DESCRIBE {full_table_name}")
    schema_result.show()
    
    # Mostra amostra dos dados
    print("\nğŸ“‹ Amostra dos dados:")
    sample_result = spark.sql(f"SELECT * FROM {full_table_name} LIMIT 5")
    sample_result.show(truncate=False)
    
    # EstatÃ­sticas por estaÃ§Ã£o
    print("\nğŸ“ˆ EstatÃ­sticas por estaÃ§Ã£o:")
    stats_result = spark.sql(f"""
        SELECT 
            estacao,
            COUNT(*) as total_medicoes,
            AVG(valorConv) as valor_medio,
            MIN(valorConv) as valor_minimo,
            MAX(valorConv) as valor_maximo
        FROM {full_table_name}
        WHERE valorConv IS NOT NULL
        GROUP BY estacao
        ORDER BY total_medicoes DESC
    """)
    stats_result.show()
else:
    print(f"âŒ Falha no salvamento: {catalog_result.get('error', 'Erro desconhecido')}")

# ============================================================================
# CÃ‰LULA 7: ExecuÃ§Ã£o do Pipeline Completo
# ============================================================================

# Executa o pipeline completo via Kedro
print("ğŸš€ Executando pipeline completo...")
print("\nPara executar o pipeline completo, use o comando:")
print("kedro run --pipeline=data_engineering")

print("\nOu para executar apenas o salvamento no catÃ¡logo:")
print("kedro run --nodes=save_to_databricks_catalog_node")

print("\nğŸ“‹ Nodes do pipeline:")
print("1. authenticate_scgas_node - AutenticaÃ§Ã£o na API")
print("2. collect_measurements_node - Coleta de dados")
print("3. create_dataframe_node - Processamento pandas")
print("4. process_with_spark_node - Processamento Spark")
print("5. save_to_databricks_catalog_node - Salvamento no catÃ¡logo (NOVO!)")

# ============================================================================
# CÃ‰LULA 8: Monitoramento e Logs
# ============================================================================

# InformaÃ§Ãµes de monitoramento
print("ğŸ“Š InformaÃ§Ãµes de Monitoramento:")
print(f"\nğŸ•’ Timestamp de execuÃ§Ã£o: {datetime.now().isoformat()}")
print(f"ğŸ”— CatÃ¡logo: {catalog_result.get('catalog', 'N/A')}")
print(f"ğŸ“‹ Tabela: {catalog_result.get('table', 'N/A')}")
print(f"ğŸ“ˆ Registros processados: {catalog_result.get('records_processed', 'N/A')}")
print(f"ğŸ’¾ Total na tabela: {catalog_result.get('total_records_in_table', 'N/A')}")
print(f"âœ… Status: {catalog_result.get('status', 'N/A')}")

if catalog_result.get("schema"):
    print(f"\nğŸ—ï¸ Schema da tabela:")
    for field in catalog_result["schema"]:
        print(f"  - {field}")

print("\nğŸ‰ Processo concluÃ­do com sucesso!")