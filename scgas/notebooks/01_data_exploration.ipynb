{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d06233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ambiente Databricks - 2025-08-26 04:46:10.242627\n",
       "Python version: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
       "Pandas version: 1.5.3\n",
       "PySpark version: 3.5.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuração inicial do ambiente Databricks\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "print(f\"Ambiente Databricks - {datetime.now()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PySpark version: {SparkSession.builder.getOrCreate().version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ce97c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configurações carregadas:\n",
       "Base URL: https://scada.scgas.com.br\n",
       "Usuário: eficienciaenergeticavega@arcelormittal.com.br\n",
       "Parâmetros de consulta: {\n",
       "  \"idIntegracao\": \"VRTA-CLT-41-023-0001\",\n",
       "  \"tagList\": [\n",
       "    \"HistTotal\"\n",
       "  ],\n",
       "  \"from\": \"2025-01-01T00:00:00.000-03:00\",\n",
       "  \"to\": \"2025-08-07T00:00:00.000-03:00\",\n",
       "  \"groupBy\": \"h\",\n",
       "  \"calcBy\": \"val\"\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configurações da API SCGAS\n",
    "# Ajuste estas configurações conforme seu ambiente\n",
    "API_CONFIG = {\n",
    "    \"base_url\": \"https://scada.scgas.com.br\",\n",
    "    \"auth_url\": \"/api/Auth/Token\",\n",
    "    \"measurement_url\": \"/api/Variable/History/Measurement\",\n",
    "    \"headers\": {\"Content-Type\": \"application/json\", \"Accept\": \"*/*\"},\n",
    "    \"timeout\": 30,\n",
    "}\n",
    "\n",
    "# Credenciais (configure via variáveis de ambiente ou secrets do Databricks)\n",
    "CREDENTIALS = {\n",
    "    \"username\": os.environ.get(\n",
    "        \"SCGAS_USERNAME\", \"eficienciaenergeticavega@arcelormittal.com.br\"\n",
    "    ),\n",
    "    \"password\": os.environ.get(\"SCGAS_PASSWORD\", \"Tk5#4Ja2\"),\n",
    "}\n",
    "\n",
    "# Parâmetros da consulta\n",
    "QUERY_PARAMS = {\n",
    "    \"idIntegracao\": \"VRTA-CLT-41-023-0001\",\n",
    "    \"tagList\": [\"HistTotal\"],\n",
    "    \"from\": \"2025-01-01T00:00:00.000-03:00\",\n",
    "    \"to\": \"2025-08-07T00:00:00.000-03:00\",\n",
    "    \"groupBy\": \"h\",\n",
    "    \"calcBy\": \"val\",\n",
    "}\n",
    "\n",
    "print(\"Configurações carregadas:\")\n",
    "print(f\"Base URL: {API_CONFIG['base_url']}\")\n",
    "print(f\"Usuário: {CREDENTIALS['username']}\")\n",
    "print(f\"Parâmetros de consulta: {json.dumps(QUERY_PARAMS, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4435945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Testando autenticação...\n",
       "URL de autenticação: https://scada.scgas.com.br/api/Auth/Token\n",
       "✅ Autenticação bem-sucedida\n",
       "Token obtido: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1laWQiO..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Função de autenticação\n",
    "def authenticate_scgas():\n",
    "    \"\"\"Autentica na API SCGAS e retorna o token de acesso.\"\"\"\n",
    "\n",
    "    auth_data = {\n",
    "        \"username\": CREDENTIALS[\"username\"],\n",
    "        \"password\": CREDENTIALS[\"password\"],\n",
    "    }\n",
    "\n",
    "    auth_url = f\"{API_CONFIG['base_url']}{API_CONFIG['auth_url']}\"\n",
    "    print(f\"URL de autenticação: {auth_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            auth_url,\n",
    "            json=auth_data,\n",
    "            headers=API_CONFIG[\"headers\"],\n",
    "            timeout=API_CONFIG[\"timeout\"],\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            token_json = response.json()\n",
    "            print(\"✅ Autenticação bem-sucedida\")\n",
    "            return token_json[\"access_token\"]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"❌ Erro ao autenticar: {response.status_code} - {response.text}\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na autenticação: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Teste de autenticação\n",
    "print(\"Testando autenticação...\")\n",
    "auth_token = authenticate_scgas()\n",
    "print(f\"Token obtido: {auth_token[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39218599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class='ansi-red-fg'>Exception</span>: ❌ Erro ao coletar dados: 401 - "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Error",
     "evalue": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m~/.ipykernel/16560/command--1-129441976:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Coleta dos dados\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColetando dados de medição...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m measurements_data \u001b[38;5;241m=\u001b[39m collect_measurements(auth_token)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTipo de dados retornados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(measurements_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(measurements_data, \u001b[38;5;28mlist\u001b[39m):\n\nFile \u001b[0;32m~/.ipykernel/16560/command--1-129441976:26\u001b[0m, in \u001b[0;36mcollect_measurements\u001b[0;34m(auth_token)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Erro ao coletar dados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Erro na coleta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\n\u001b[0;31mException\u001b[0m: ❌ Erro ao coletar dados: 401 - ",
     "output_type": "error",
     "traceback": [
      "Error: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n",
      "File \u001b[0;32m~/.ipykernel/16560/command--1-129441976:37\u001b[0m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Coleta dos dados\u001b[39;00m\n",
      "\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColetando dados de medição...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 37\u001b[0m measurements_data \u001b[38;5;241m=\u001b[39m collect_measurements(auth_token)\n",
      "\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTipo de dados retornados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(measurements_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(measurements_data, \u001b[38;5;28mlist\u001b[39m):\n",
      "\n",
      "File \u001b[0;32m~/.ipykernel/16560/command--1-129441976:26\u001b[0m, in \u001b[0;36mcollect_measurements\u001b[0;34m(auth_token)\u001b[0m\n",
      "\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n",
      "\u001b[1;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Erro ao coletar dados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m     28\u001b[0m         )\n",
      "\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Erro na coleta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mException\u001b[0m: ❌ Erro ao coletar dados: 401 - \n",
      "    at p._doExecution (/Users/amaro/.cursor/extensions/paiqo.databricks-vscode-2.8.0-universal/dist/node/extension.js:2:68482)\n",
      "    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n",
      "    at p.executeHandler (/Users/amaro/.cursor/extensions/paiqo.databricks-vscode-2.8.0-universal/dist/node/extension.js:2:56692)\n",
      "    at fhe.$executeCells (file:///Applications/Cursor.app/Contents/Resources/app/out/vs/workbench/api/node/extensionHostProcess.js:198:39239)"
     ]
    }
   ],
   "source": [
    "# Função de coleta de dados\n",
    "def collect_measurements(auth_token):\n",
    "    \"\"\"Coleta dados de medição usando o token de autenticação.\"\"\"\n",
    "\n",
    "    # Prepara headers com o token\n",
    "    headers = API_CONFIG[\"headers\"].copy()\n",
    "    headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n",
    "\n",
    "    data_url = f\"{API_CONFIG['base_url']}{API_CONFIG['measurement_url']}\"\n",
    "\n",
    "    print(f\"URL de coleta: {data_url}\")\n",
    "    print(f\"Body da requisição: {json.dumps(QUERY_PARAMS, indent=2)}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            data_url, json=QUERY_PARAMS, headers=headers, timeout=API_CONFIG[\"timeout\"]\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\n",
    "                f\"✅ Dados coletados com sucesso. Registros: {len(data) if isinstance(data, list) else 'N/A'}\"\n",
    "            )\n",
    "            return data\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"❌ Erro ao coletar dados: {response.status_code} - {response.text}\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na coleta: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Coleta dos dados\n",
    "print(\"Coletando dados de medição...\")\n",
    "measurements_data = collect_measurements(auth_token)\n",
    "print(f\"Tipo de dados retornados: {type(measurements_data)}\")\n",
    "if isinstance(measurements_data, list):\n",
    "    print(\n",
    "        f\"Primeiro registro: {json.dumps(measurements_data[0], indent=2) if measurements_data else 'Lista vazia'}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e779c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar DataFrame pandas\n",
    "def create_pandas_dataframe(measurements_data):\n",
    "    \"\"\"Cria um DataFrame pandas a partir dos dados da API.\"\"\"\n",
    "\n",
    "    print(\"Processando dados da API...\")\n",
    "\n",
    "    if isinstance(measurements_data, list):\n",
    "        # Processa cada item da lista\n",
    "        processed_data = []\n",
    "        for item in measurements_data:\n",
    "            if isinstance(item, dict):\n",
    "                processed_data.append(\n",
    "                    {\n",
    "                        \"codVar\": item.get(\"codVar\", \"\"),\n",
    "                        \"tag\": item.get(\"tag\", \"\"),\n",
    "                        \"idIntegracao\": item.get(\"idIntegracao\", \"\"),\n",
    "                        \"unidade\": item.get(\"unidade\", \"\"),\n",
    "                        \"descricao\": item.get(\"descricao\", \"\"),\n",
    "                        \"data\": item.get(\"data\", \"\"),\n",
    "                        \"valorConv\": item.get(\"valorConv\", 0.0),\n",
    "                        \"valorConvFormat\": item.get(\"valorConvFormat\", 0.0),\n",
    "                        \"estacao\": item.get(\"estacao\", \"\"),\n",
    "                        \"codEst\": item.get(\"codEst\", \"\"),\n",
    "                        \"codMed\": item.get(\"codMed\", \"\"),\n",
    "                        \"intervaloLeituraMin\": item.get(\"intervaloLeituraMin\", 0),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "    else:\n",
    "        # Se for um dicionário único\n",
    "        df = pd.DataFrame([measurements_data])\n",
    "\n",
    "    # Conversões de tipo\n",
    "    if \"data\" in df.columns:\n",
    "        df[\"data\"] = pd.to_datetime(df[\"data\"], errors=\"coerce\")\n",
    "\n",
    "    numeric_columns = [\n",
    "        \"valorConv\",\n",
    "        \"valorConvFormat\",\n",
    "        \"codVar\",\n",
    "        \"codEst\",\n",
    "        \"codMed\",\n",
    "        \"intervaloLeituraMin\",\n",
    "    ]\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    print(f\"DataFrame pandas criado com {len(df)} linhas e {len(df.columns)} colunas\")\n",
    "    print(f\"Colunas: {list(df.columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Criação do DataFrame pandas\n",
    "print(\"Criando DataFrame pandas...\")\n",
    "pandas_df = create_pandas_dataframe(measurements_data)\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "display(pandas_df.head())\n",
    "\n",
    "print(\"\\nEstatísticas dos valores:\")\n",
    "if \"valorConv\" in pandas_df.columns:\n",
    "    print(f\"Valor médio: {pandas_df['valorConv'].mean():,.2f} m³\")\n",
    "    print(f\"Valor mínimo: {pandas_df['valorConv'].min():,.2f} m³\")\n",
    "    print(f\"Valor máximo: {pandas_df['valorConv'].max():,.2f} m³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc274224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar DataFrame Spark\n",
    "def create_spark_dataframe(measurements_data):\n",
    "    \"\"\"Cria um DataFrame Spark a partir dos dados da API.\"\"\"\n",
    "\n",
    "    print(\"Criando DataFrame Spark...\")\n",
    "\n",
    "    # Inicializa Spark\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"SCGAS_Measurements_Databricks\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    print(f\"Sessão Spark inicializada: {spark.version}\")\n",
    "\n",
    "    # Define schema\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"codVar\", StringType(), True),\n",
    "            StructField(\"tag\", StringType(), True),\n",
    "            StructField(\"idIntegracao\", StringType(), True),\n",
    "            StructField(\"unidade\", StringType(), True),\n",
    "            StructField(\"descricao\", StringType(), True),\n",
    "            StructField(\"data\", StringType(), True),\n",
    "            StructField(\"valorConv\", DoubleType(), True),\n",
    "            StructField(\"valorConvFormat\", DoubleType(), True),\n",
    "            StructField(\"estacao\", StringType(), True),\n",
    "            StructField(\"codEst\", StringType(), True),\n",
    "            StructField(\"codMed\", StringType(), True),\n",
    "            StructField(\"intervaloLeituraMin\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Processa dados\n",
    "    if isinstance(measurements_data, list):\n",
    "        processed_data = []\n",
    "        for item in measurements_data:\n",
    "            if isinstance(item, dict):\n",
    "                processed_data.append(\n",
    "                    (\n",
    "                        str(item.get(\"codVar\", \"\")),\n",
    "                        str(item.get(\"tag\", \"\")),\n",
    "                        str(item.get(\"idIntegracao\", \"\")),\n",
    "                        str(item.get(\"unidade\", \"\")),\n",
    "                        str(item.get(\"descricao\", \"\")),\n",
    "                        str(item.get(\"data\", \"\")),\n",
    "                        float(item.get(\"valorConv\", 0.0)),\n",
    "                        float(item.get(\"valorConvFormat\", 0.0)),\n",
    "                        str(item.get(\"estacao\", \"\")),\n",
    "                        str(item.get(\"codEst\", \"\")),\n",
    "                        str(item.get(\"codMed\", \"\")),\n",
    "                        float(item.get(\"intervaloLeituraMin\", 0)),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Cria DataFrame Spark\n",
    "        spark_df = spark.createDataFrame(processed_data, schema)\n",
    "    else:\n",
    "        # DataFrame com uma linha\n",
    "        spark_df = spark.createDataFrame([measurements_data], schema)\n",
    "\n",
    "    print(f\"DataFrame Spark criado com {spark_df.count()} linhas\")\n",
    "    print(\"Schema do DataFrame:\")\n",
    "    spark_df.printSchema()\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "\n",
    "# Criação do DataFrame Spark\n",
    "spark_df = create_spark_dataframe(measurements_data)\n",
    "print(\"\\nPrimeiras 5 linhas do DataFrame Spark:\")\n",
    "display(spark_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d41d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise exploratória com Spark SQL\n",
    "print(\"Análise exploratória com Spark SQL:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Registra o DataFrame como view temporária\n",
    "spark_df.createOrReplaceTempView(\"scgas_measurements\")\n",
    "\n",
    "# Estatísticas básicas\n",
    "print(\"1. Estatísticas dos valores de medição:\")\n",
    "stats_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_registros,\n",
    "    AVG(valorConv) as valor_medio,\n",
    "    MIN(valorConv) as valor_minimo,\n",
    "    MAX(valorConv) as valor_maximo,\n",
    "    STDDEV(valorConv) as desvio_padrao\n",
    "FROM scgas_measurements\n",
    "\"\"\"\n",
    "stats_result = spark.sql(stats_query)\n",
    "display(stats_result.toPandas())\n",
    "\n",
    "# Análise por estação\n",
    "print(\"\\n2. Análise por estação:\")\n",
    "estacao_query = \"\"\"\n",
    "SELECT \n",
    "    estacao,\n",
    "    COUNT(*) as total_registros,\n",
    "    AVG(valorConv) as valor_medio,\n",
    "    COUNT(DISTINCT codVar) as variaveis_unicas\n",
    "FROM scgas_measurements \n",
    "GROUP BY estacao\n",
    "ORDER BY total_registros DESC\n",
    "\"\"\"\n",
    "estacao_result = spark.sql(estacao_query)\n",
    "display(estacao_result.toPandas())\n",
    "\n",
    "# Análise temporal\n",
    "print(\"\\n3. Análise temporal:\")\n",
    "temporal_query = \"\"\"\n",
    "SELECT \n",
    "    DATE(data) as data_medicao,\n",
    "    COUNT(*) as registros_dia,\n",
    "    AVG(valorConv) as valor_medio_dia\n",
    "FROM scgas_measurements \n",
    "WHERE data IS NOT NULL\n",
    "GROUP BY DATE(data)\n",
    "ORDER BY data_medicao\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "temporal_result = spark.sql(temporal_query)\n",
    "display(temporal_result.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamento dos dados no Databricks\n",
    "print(\"Salvando dados no Databricks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define caminhos de destino (ajuste conforme seu ambiente)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_path = f\"/dbfs/FileStore/scgas/measurements_{timestamp}\"\n",
    "\n",
    "# Salva como Parquet (recomendado para Spark)\n",
    "parquet_path = f\"{base_path}/measurements.parquet\"\n",
    "print(f\"Salvando DataFrame Spark como Parquet: {parquet_path}\")\n",
    "spark_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "print(\"✅ DataFrame Spark salvo como Parquet\")\n",
    "\n",
    "# Salva como CSV (para compatibilidade)\n",
    "csv_path = f\"{base_path}/measurements.csv\"\n",
    "print(f\"Salvando DataFrame pandas como CSV: {csv_path}\")\n",
    "pandas_df.to_csv(csv_path, index=False)\n",
    "print(\"✅ DataFrame pandas salvo como CSV\")\n",
    "\n",
    "# Salva dados brutos como JSON\n",
    "json_path = f\"{base_path}/raw_data.json\"\n",
    "print(f\"Salvando dados brutos como JSON: {json_path}\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(measurements_data, f, indent=2)\n",
    "print(\"✅ Dados brutos salvos como JSON\")\n",
    "\n",
    "print(f\"\\n📁 Todos os arquivos salvos em: {base_path}\")\n",
    "print(f\"📊 Total de registros processados: {len(measurements_data):,}\")\n",
    "print(f\"⏰ Timestamp de execução: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c28c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final da execução\n",
    "print(\"🎯 RESUMO FINAL DA EXECUÇÃO NO DATABRICKS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Autenticação: Sucesso\")\n",
    "print(f\"✅ Coleta de dados: {len(measurements_data):,} registros\")\n",
    "print(\n",
    "    f\"✅ DataFrame pandas: {pandas_df.shape[0]:,} linhas x {pandas_df.shape[1]} colunas\"\n",
    ")\n",
    "print(f\"✅ DataFrame Spark: {spark_df.count():,} linhas\")\n",
    "print(f\"✅ Salvamento: Concluído em {base_path}\")\n",
    "print(f\"\\n🚀 Pipeline executado com sucesso no Databricks!\")\n",
    "print(f\"📈 Dados prontos para análise e processamento adicional.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análises adicionais com Spark SQL\n",
    "print(\"🔍 Análises adicionais:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Análise de tendências diárias\n",
    "print(\"1. Tendências diárias:\")\n",
    "trend_query = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('day', data) as dia,\n",
    "    AVG(valorConv) as valor_medio_dia,\n",
    "    COUNT(*) as registros,\n",
    "    STDDEV(valorConv) as variabilidade\n",
    "FROM scgas_measurements \n",
    "WHERE data IS NOT NULL\n",
    "GROUP BY DATE_TRUNC('day', data)\n",
    "ORDER BY dia\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "trends = spark.sql(trend_query)\n",
    "display(trends.toPandas())\n",
    "\n",
    "# Análise de qualidade dos dados\n",
    "print(\"\\n2. Qualidade dos dados:\")\n",
    "quality_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_registros,\n",
    "    COUNT(CASE WHEN valorConv IS NOT NULL THEN 1 END) as registros_com_valor,\n",
    "    COUNT(CASE WHEN data IS NOT NULL THEN 1 END) as registros_com_data,\n",
    "    COUNT(CASE WHEN estacao IS NOT NULL THEN 1 END) as registros_com_estacao,\n",
    "    COUNT(CASE WHEN valorConv = 0 THEN 1 END) as registros_zero\n",
    "FROM scgas_measurements\n",
    "\"\"\"\n",
    "quality = spark.sql(quality_query)\n",
    "display(quality.toPandas())\n",
    "\n",
    "# Análise de distribuição dos valores\n",
    "print(\"\\n3. Distribuição dos valores:\")\n",
    "distribution_query = \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN valorConv < 1000000 THEN 'Baixo (< 1M)'\n",
    "        WHEN valorConv < 5000000 THEN 'Médio (1M-5M)'\n",
    "        WHEN valorConv < 10000000 THEN 'Alto (5M-10M)'\n",
    "        ELSE 'Muito Alto (> 10M)'\n",
    "    END as faixa_valor,\n",
    "    COUNT(*) as quantidade,\n",
    "    AVG(valorConv) as valor_medio_faixa\n",
    "FROM scgas_measurements \n",
    "WHERE valorConv IS NOT NULL\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN valorConv < 1000000 THEN 'Baixo (< 1M)'\n",
    "        WHEN valorConv < 5000000 THEN 'Médio (1M-5M)'\n",
    "        WHEN valorConv < 10000000 THEN 'Alto (5M-10M)'\n",
    "        ELSE 'Muito Alto (> 10M)'\n",
    "    END\n",
    "ORDER BY quantidade DESC\n",
    "\"\"\"\n",
    "distribution = spark.sql(distribution_query)\n",
    "display(distribution.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações dos dados\n",
    "print(\"📊 Criando visualizações:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "  \n",
    "    # Configurações de visualização\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "  \n",
    "    # Figura 1: Distribuição dos valores\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "  \n",
    "    # Histograma dos valores\n",
    "    axes[0,0].hist(pandas_df['valorConv'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('Distribuição dos Valores de Medição')\n",
    "    axes[0,0].set_xlabel('Valor (m³)')\n",
    "    axes[0,0].set_ylabel('Frequência')\n",
    "  \n",
    "    # Boxplot dos valores\n",
    "    axes[0,1].boxplot(pandas_df['valorConv'])\n",
    "    axes[0,1].set_title('Boxplot dos Valores de Medição')\n",
    "    axes[0,1].set_ylabel('Valor (m³)')\n",
    "  \n",
    "    # Série temporal dos valores\n",
    "    if 'data' in pandas_df.columns:\n",
    "        pandas_df_sorted = pandas_df.sort_values('data')\n",
    "        axes[1,0].plot(pandas_df_sorted['data'], pandas_df_sorted['valorConv'], alpha=0.7)\n",
    "        axes[1,0].set_title('Evolução Temporal dos Valores')\n",
    "        axes[1,0].set_xlabel('Data')\n",
    "        axes[1,0].set_ylabel('Valor (m³)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "  \n",
    "    # Comparação valorConv vs valorConvFormat\n",
    "    axes[1,1].scatter(pandas_df['valorConv'], pandas_df['valorConvFormat'], alpha=0.6)\n",
    "    axes[1,1].plot([pandas_df['valorConv'].min(), pandas_df['valorConv'].max()], \n",
    "                    [pandas_df['valorConv'].min(), pandas_df['valorConv'].max()], 'r--', alpha=0.8)\n",
    "    axes[1,1].set_title('Valor Convertido vs Valor Formatado')\n",
    "    axes[1,1].set_xlabel('Valor Convertido (m³)')\n",
    "    axes[1,1].set_ylabel('Valor Formatado (m³)')\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  \n",
    "    print(\"✅ Visualizações criadas com sucesso!\")\n",
    "  \n",
    "except ImportError:\n",
    "    print(\"⚠️ Matplotlib/Seaborn não disponível. Pulando visualizações.\")\n",
    "    print(\"Para visualizações, instale: pip install matplotlib seaborn\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
