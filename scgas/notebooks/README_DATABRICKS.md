# Pipeline SCGAS para Databricks

Este diret√≥rio cont√©m notebooks e scripts para executar o pipeline de coleta de dados da API SCGAS no ambiente Databricks.

## üìÅ Arquivos Dispon√≠veis

### 1. `02_databricks_pipeline.ipynb`
- **Notebook Jupyter** para execu√ß√£o interativa no Databricks
- **C√©lulas organizadas** para cada etapa do pipeline
- **Visualiza√ß√µes** e an√°lises explorat√≥rias
- **Execu√ß√£o passo a passo** com coment√°rios detalhados

### 2. `databricks_scgas_pipeline.py`
- **Script Python standalone** para execu√ß√£o automatizada
- **Pipeline completo** em um √∫nico arquivo
- **Execu√ß√£o via job** ou notebook
- **Logs estruturados** e tratamento de erros

## üöÄ Como Executar no Databricks

### Op√ß√£o 1: Notebook Interativo

1. **Fa√ßa upload** do arquivo `02_databricks_pipeline.ipynb` para seu workspace
2. **Anexe a um cluster** com as seguintes especifica√ß√µes:
   - **Runtime**: Databricks Runtime 10.4 LTS ou superior
   - **Python**: 3.9+
   - **Bibliotecas**: requests, pandas, pyspark (j√° inclu√≠das)
3. **Execute as c√©lulas** sequencialmente

### Op√ß√£o 2: Script Python

1. **Fa√ßa upload** do arquivo `databricks_scgas_pipeline.py`
2. **Crie um notebook** e execute:
   ```python
   %run /path/to/databricks_scgas_pipeline.py
   ```
3. **Ou execute via job** do Databricks

### Op√ß√£o 3: Job Automatizado

1. **Crie um novo job** no Databricks
2. **Adicione uma task** do tipo "Python Script"
3. **Configure o script** para executar `databricks_scgas_pipeline.py`
4. **Agende a execu√ß√£o** conforme necess√°rio

## ‚öôÔ∏è Configura√ß√£o do Ambiente

### Vari√°veis de Ambiente

Configure as credenciais via **Secrets do Databricks**:

```python
# No notebook ou script
import os
from pyspark.sql import SparkSession

# Configurar secrets
spark.conf.set("spark.databricks.secrets.scope", "scgas-secrets")
spark.conf.set("spark.databricks.secrets.key", "scgas-credentials")

# Ou via vari√°veis de ambiente
os.environ["SCGAS_USERNAME"] = "seu_usuario"
os.environ["SCGAS_PASSWORD"] = "sua_senha"
```

### Secrets do Databricks

1. **Acesse** Admin Console > Secrets
2. **Crie um scope** chamado `scgas-secrets`
3. **Adicione as chaves**:
   - `scgas-username`: seu usu√°rio da API
   - `scgas-password`: sua senha da API

## üìä Estrutura de Dados

### Campos Coletados

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `codVar` | String | C√≥digo da vari√°vel |
| `tag` | String | Tag de identifica√ß√£o |
| `idIntegracao` | String | ID de integra√ß√£o |
| `unidade` | String | Unidade de medida |
| `descricao` | String | Descri√ß√£o da medi√ß√£o |
| `data` | Timestamp | Data/hora da medi√ß√£o |
| `valorConv` | Double | Valor convertido |
| `valorConvFormat` | Double | Valor formatado |
| `estacao` | String | Esta√ß√£o de medi√ß√£o |
| `codEst` | String | C√≥digo da esta√ß√£o |
| `codMed` | String | C√≥digo da medi√ß√£o |
| `intervaloLeituraMin` | Double | Intervalo de leitura |

### Formato de Sa√≠da

Os dados s√£o salvos em tr√™s formatos:

1. **Parquet** (`measurements.parquet`) - Otimizado para Spark
2. **CSV** (`measurements.csv`) - Compat√≠vel com pandas
3. **JSON** (`raw_data.json`) - Dados brutos da API

## üîß Personaliza√ß√µes

### Par√¢metros de Consulta

Edite a fun√ß√£o `load_configuration()` para ajustar:

```python
QUERY_PARAMS = {
    "idIntegracao": "SEU_ID_INTEGRACAO",
    "tagList": ["SUA_TAG"],
    "from": "2025-01-01T00:00:00.000-03:00",
    "to": "2025-08-07T00:00:00.000-03:00",
    "groupBy": "h",  # h=hor√°rio, d=dia, m=m√™s
    "calcBy": "val"  # val=valor, avg=m√©dia
}
```

### Configura√ß√µes da API

Ajuste URLs e timeouts:

```python
API_CONFIG = {
    "base_url": "https://sua-api.com",
    "timeout": 60,  # segundos
    "retry_attempts": 3
}
```

## üìà An√°lises Dispon√≠veis

### An√°lise Explorat√≥ria

O pipeline executa automaticamente:

1. **Estat√≠sticas b√°sicas** dos valores de medi√ß√£o
2. **An√°lise por esta√ß√£o** de monitoramento
3. **An√°lise temporal** dos dados
4. **Qualidade dos dados** e consist√™ncia

### Queries SQL Personalizadas

Use o DataFrame Spark para an√°lises adicionais:

```python
# Exemplo: An√°lise de tend√™ncias
trend_query = """
SELECT 
    DATE_TRUNC('day', data) as dia,
    AVG(valorConv) as valor_medio_dia,
    COUNT(*) as registros
FROM scgas_measurements 
GROUP BY DATE_TRUNC('day', data)
ORDER BY dia
"""
trends = spark.sql(trend_query)
trends.show()
```

## üö® Troubleshooting

### Problemas Comuns

1. **Erro de autentica√ß√£o**
   - Verifique credenciais
   - Confirme se a API est√° acess√≠vel

2. **Timeout na coleta**
   - Aumente o valor de `timeout`
   - Verifique conectividade de rede

3. **Erro de mem√≥ria**
   - Aumente recursos do cluster
   - Processe dados em lotes menores

### Logs e Debug

O script gera logs detalhados:

```
üöÄ Configurando ambiente Databricks...
‚úÖ Spark inicializado: 3.4.1
üîê Autenticando na API SCGAS...
‚úÖ Autentica√ß√£o bem-sucedida
üìä Coletando dados de medi√ß√£o...
‚úÖ Dados coletados com sucesso. Registros: 2,544
```

## üìû Suporte

Para d√∫vidas ou problemas:

1. **Verifique os logs** de execu√ß√£o
2. **Confirme as configura√ß√µes** da API
3. **Teste a conectividade** de rede
4. **Verifique as permiss√µes** do cluster

---

**Pipeline SCGAS para Databricks** - Vers√£o 1.0  
**Data**: 2025-08-25  
**Autor**: Equipe de Engenharia de Dados
